//===- IntrinsicsGenX.td - Defines GenX intrinsics -----------*- tablegen -*-===//
//
//      Copyright (c) 2014 Intel Corporation.
//      All rights reserved.
//
//        INTEL CORPORATION PROPRIETARY INFORMATION
//
// This software is supplied under the terms of a license
// agreement or nondisclosure agreement with Intel Corp.
// and may not be copied or disclosed except in accordance
// with the terms of that agreement.
//
//===----------------------------------------------------------------------===//
//
// This file defines all of the GenX-specific intrinsics, which correspond to
// vISA instructions.
//
// Comment lines with a triple slash /// introduction are extracted and
// appended to docs/Targets/GenX/GenXLangRef.rst to give the GenX backend
// language reference in docs/autogenerated/Targets/GenX/GenXLangRef.rst.
//
//===----------------------------------------------------------------------===//

let TargetPrefix = "genx" in {  // All intrinsics start with "llvm.genx.".

  //--------------------------------------------------------------------
  // Start and end markers of the genx intrinsic enum values. This relies on
  // tablegen outputting the intrinsics in sorted by name order.
  def int_genx_aaaabegin : Intrinsic<[llvm_anyvector_ty], [], []>;
  def int_genx_zzzzend : Intrinsic<[llvm_anyvector_ty], [], []>;

  // --------------------------------
  /// Region/element access intrinsics
  /// --------------------------------
  ///
  /// ``llvm.genx.rdregion`` : read a region, direct or single-indirect
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: vector to read region out of
  /// * arg1: i32 vstride in elements, constant
  /// * arg2: i32 width in elements, constant
  /// * arg3: i32 stride in elements, constant
  /// * arg4: i16 or vXi16 offset in bytes
  /// * arg5: i32 parent width, constant, ignored if offset is constant
  ///
  /// * Return value: the region extracted
  ///
  /// The return type must be a vector with the same element type as the input
  /// vector, and number of elements giving the total size of the region.
  /// The element type must be an integral power of two number of bytes up to
  /// and including 8 bytes in size, thus one of i8, i16, i32, i64, half,
  /// float, double. In particular i1 is not allowed.
  /// The width must be non-zero and must divide the total size evenly.
  ///
  /// There is no requirement on vstride, width, stride or total size being
  /// a power of two or having any maximum.
  ///
  /// The offset in bytes arg can be i16 or vector of i16. If a vector, then
  /// its vector width must be the height of the region, i.e. the total
  /// size of the region divided by the width.
  ///
  // This paragraph does not have a triple slash line introduction, so it is
  // not part of the GenX language reference:
  // After lowering, the return type can be a scalar of the same type as the
  // element type of the input, indicating that the region has one element.
  // (Lowering lowers an extractelement to this type of rdregion.)
  //
  /// The parent width arg is ignored if the offset arg is constant. If the
  /// offset arg is variable, then a non-undef parent width is a statement
  /// that the value of offset is such that a row of the region does not
  /// cross a multiple of parent width boundary. This is used by the backend
  /// to determine whether the region can be collapsed into another region.
  ///
  def int_genx_rdregion : Intrinsic<[llvm_anyvector_ty], [llvm_anyvector_ty,
      llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, llvm_i32_ty],
      [IntrNoMem]>;

  /// ``llvm.genx.wrregion`` : write a region, direct or single-indirect
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: vector to write region in to
  /// * arg1: subvector to write into the region
  /// * arg2: i32 vstride in elements, constant
  /// * arg3: i32 width in elements, constant
  /// * arg4: i32 stride in elements, constant
  /// * arg5: i16 or vXi16 offset in bytes
  /// * arg6: i32 parent width, constant, ignored if offset is constant
  /// * arg7: vector of i1 mask, or scalar i1
  ///
  /// * Return value: the updated vector with the region modified
  ///
  /// The return type must be a vector with the same type as the arg0 vector.
  /// The arg1 subvector must have the same element type as the arg0 vector
  /// and be no larger.
  /// The element type must be an integral power of two number of bytes up to
  /// and including 8 bytes in size, thus one of i8, i16, i32, i64, half,
  /// float, double. In particular i1 is not allowed.
  /// The width must be non-zero and must divide the total size evenly.
  ///
  /// The arg7 mask is a vector of booleans, at least as wide as the
  /// * arg1 subvector, such that an element of the subvector is written into
  /// its place in the vector only if the corresponding element of the mask
  /// is true.
  /// Alternatively, arg7 can be a single i1 constant with value 1,
  /// meaning that the wrregion is unconditional.
  /// Alternatively, arg7 can be a v32i1. If the arg1 subvector is wider than
  /// 32 elements, then the mask is replicated enough times to cover the
  /// required number of elements.
  ///
  /// There is no requirement on vstride, width, stride or total size being
  /// a power of two or having any maximum.
  ///
  /// The offset in bytes arg can be i16 or vector of i16. If a vector, then
  /// its vector width must be the height of the region, i.e. the total
  /// size of the region divided by the width.
  ///
  /// After lowering, the arg1 subvector to write can be a scalar of the same
  /// type as an element of arg0, indicating that the region has one element.
  /// (Lowering lowers an insertelement to this type of wrregion.)
  ///
  /// The parent width arg is ignored if the offset arg is constant. If the
  /// offset arg is variable, then a non-undef parent width is a statement
  /// that the value of offset is such that a row of the region does not
  /// cross a multiple of parent width boundary. This is used by the backend
  /// to determine whether the region can be collapsed into another region.
  ///
  def int_genx_wrregion : Intrinsic<[llvm_anyvector_ty], [LLVMMatchType<0>,
      llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty,
      llvm_i32_ty, llvm_anyint_ty], [IntrNoMem]>;

  // ------------------------------
  /// ALU type conversion intrinsics
  /// ------------------------------

  /// ``llvm.genx.fptosi.sat`` : convert floating point to signed integer with saturate
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: value to saturate, any scalar or vector floating point type
  ///
  /// * Return value: converted value, any scalar or vector integer type
  ///               (treated as signed) with same vector width as arg0
  ///
  def int_genx_fptosi_sat :
              Intrinsic<[llvm_anyint_ty], [llvm_anyfloat_ty], [IntrNoMem]>;

  /// ``llvm.genx.fptoui.sat`` : convert floating point to unsigned integer with saturate
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: value to saturate, any scalar or vector floating point type
  ///
  /// * Return value: converted value, any scalar or vector integer type
  ///               (treated as unsigned) with same vector width as arg0
  ///
  def int_genx_fptoui_sat :
              Intrinsic<[llvm_anyint_ty], [llvm_anyfloat_ty], [IntrNoMem]>;

  /// ``llvm.genx.sat`` : floating point saturate
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: value to saturate, any scalar or vector floating point type
  ///
  /// * Return value: saturated value, same type as arg0
  ///
  /// We represent floating point saturation by simply calling this intrinsic
  /// on the result of a floating point operation. This works because the
  /// value before saturation fits in the same type.
  ///
  /// We do not have an equivalent for integer saturation, because the
  /// before-saturation value needs a bigger integer type than the result.
  /// Instead, any integer operation that supports saturation needs an
  /// intrinsic for the saturating variant.
  ///
  def int_genx_sat :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*trunc.sat`` : integer truncation with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sstrunc.sat`` : signed result, signed operand
  /// * ``llvm.genx.sutrunc.sat`` : signed result, unsigned operand
  /// * ``llvm.genx.ustrunc.sat`` : unsigned result, signed operand
  /// * ``llvm.genx.uutrunc.sat`` : unsigned result, unsigned operand
  ///
  /// * arg0: value to truncate, any scalar or vector integer type
  ///
  /// * Return value: truncated value, any scalar or vector integer type
  ///               with same vector width as arg0
  ///
  def int_genx_sstrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;
  def int_genx_sutrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;
  def int_genx_ustrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;
  def int_genx_uutrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;

  // -------------------
  /// Modifier intrinsics
  /// -------------------
  ///
  /// Abs is the only source modifier that is represented
  /// by an intrinsic; neg(x) uses 0-x, and not(x) uses x^-1.
  /// (x^-1 is lowered to the backend-internal notp/noti intrinsics in
  /// GenXLowering.)
  ///
  /// ``llvm.genx.abs*`` : take absolute value
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.absf`` : abs modifier for fp
  /// * ``llvm.genx.absi`` : abs modifier for integer
  ///
  /// * arg0: input value, scalar/vector
  ///
  /// * Return value: result, same type
  ///
  def int_genx_absf :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_absi :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // ----------------------------
  /// Boolean reduction intrinsics
  /// ----------------------------

  /// ``llvm.genx.all`` : true if all input elements are true
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value: v*i1
  ///
  /// * Return value: i1 result
  ///
  def int_genx_all : Intrinsic<[llvm_i1_ty], [llvm_anyint_ty], [IntrNoMem]>;

  /// ``llvm.genx.any`` : true if any input element is true
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value: v*i1
  ///
  /// * Return value: i1 result
  ///
  def int_genx_any : Intrinsic<[llvm_i1_ty], [llvm_anyint_ty], [IntrNoMem]>;

  // --------------
  /// ALU intrinsics
  /// --------------

  /// ``llvm.genx.acos`` : acos instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, scalar/vector half/float 
  ///
  /// * Return value: result, same type
  ///
  def int_genx_acos :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*add`` : integer add instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssadd`` : result signed, operands signed
  /// * ``llvm.genx.suadd`` : result signed, operands unsigned
  /// * ``llvm.genx.usadd`` : result unsigned, operands signed
  /// * ``llvm.genx.uuadd`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width
  ///
  /// The signedness of a non-saturating add only makes a difference if the
  /// element type of the result is bigger than the element type of the args.
  ///
  /// For an integer add where the result and the sources are the same type,
  /// and is not saturating, use the LLVM IR Add instruction.
  ///
  /// For an fp add, use the LLVM IR FAdd instruction, followed by
  /// llvm.genx.sat if saturation is required.
  ///
  def int_genx_ssadd : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_suadd : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usadd : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uuadd : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*add.sat`` : add instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssadd.sat`` : result signed, operands signed
  /// * ``llvm.genx.suadd.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usadd.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uuadd.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width
  ///
  def int_genx_ssadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_suadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uuadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// addc
  /// ^^^^
  /// No intrinsic for addc as it has two results.
  ///

  /// ``llvm.genx.asin`` : asin instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_asin :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// asr
  /// ^^^
  /// asr intrinsic is not needed. Because asr cannot overflow, an asr that
  /// saturates with a smaller result type than the execution type can be
  /// represented by an LLVM IR Asr instruction then an llvm.genx.sstrunc.sat.
  ///

  /// ``llvm.genx.atan`` : atan instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_atan :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*avg`` : integer averaging, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssavg`` : result signed, operands signed
  /// * ``llvm.genx.suavg`` : result signed, operands unsigned
  /// * ``llvm.genx.usavg`` : result unsigned, operands signed
  /// * ``llvm.genx.uuavg`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type (not i64)
  ///               with same vector width
  ///
  def int_genx_ssavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_suavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uuavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*avg.sat`` : integer averaging with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssavg.sat`` : result signed, operands signed
  /// * ``llvm.genx.suavg.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usavg.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uuavg.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type (not i64)
  ///               with same vector width
  ///
  def int_genx_ssavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_suavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uuavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*bfe`` : bitfield extract
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sbfe`` : bitfield extract, signed result
  /// * ``llvm.genx.ubfe`` : bitfield extract, unsigned result
  ///
  /// * arg0: first input, any scalar/vector i32 type
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as arg0
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_sbfe : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>,
                LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_ubfe : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>,
                LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.bfi`` : bitfield insert
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector i32 type
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as arg0
  /// * arg3: fourth input, same type as arg0
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_bfi : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>,
        LLVMMatchType<0>, LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.bfrev`` : reverse bits
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector i32 type
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_bfrev : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.cbit`` : count set bits
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector i32 type
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_cbit : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// cmp
  /// ^^^
  /// No intrinsic needed as the LLVM IR ICmp and FCmp instructions cover
  /// vISA functionality
  ///

  /// ``llvm.genx.cos`` : cos instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_cos :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// div
  /// ^^^
  /// No intrinsic needed as the LLVM IR SDiv, UDiv and FDiv instructions
  /// cover vISA functionality
  ///

  /// ``llvm.genx.dp2`` : dp2 instruction (dot product on groups of 4 elements)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dp2 :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.dp3`` : dp3 instruction (dot product on groups of 3 elements)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dp3 :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.dp4`` : dp4 instruction (dot product on groups of 4 elements)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dp4 :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.dph`` : dph instruction (dot product homogenous)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dph :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.exp`` : base 2 exponent
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_exp :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*fbh`` : find bit high
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sfbh`` : find bit high, signed operand
  /// * ``llvm.genx.ufbh`` : find bit high, unsigned operand
  ///
  /// * arg0: input value, any scalar/vector i32 type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_sfbh :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_ufbh :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.fbl`` : find bit low
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector i32 type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_fbl :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.frc`` : fractional part
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_frc :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.inv`` : reciprocal
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_inv :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.inv.ieee`` : reciprocal, IEEE result
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_inv_ieee :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.line`` : linear equation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, vector float with a multiple of 4 elements
  /// * arg1: second input value, vector float, same size
  ///
  /// * Return value: result, same type as the args
  ///
  /// The coefficients for the line operation are arg0[4n+0] and arg0[4n+3].
  /// Which n you get for each 4 wide part of the operation is
  /// undefined, because of the effect of legalization in the GenX backend.
  ///
  /// Therefore, to get the correct effect, arg0 should be the result of a
  /// rdregion<0;4,1> (i.e. replicate a single row of 4 elements) whose input is
  /// a 4 wide vector, so the same arg0[0], arg0[1] and arg0[3] are guaranteed
  /// to be used.
  ///
  def int_genx_line : Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.log`` : base 2 logarithm
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_log :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.lrp`` : linear interpolation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  /// * arg2: third input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_lrp : Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>,
              LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.lzd`` : leading zero detection
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector i32 type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_lzd :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*mad`` : mad instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmad`` : result signed, operands signed
  /// * ``llvm.genx.sumad`` : result signed, operands unsigned
  /// * ``llvm.genx.usmad`` : result unsigned, operands signed
  /// * ``llvm.genx.uumad`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width
  ///
  def int_genx_ssmad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sumad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usmad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uumad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*mad.sat`` : mad instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmad.sat`` : result signed, operands signed
  /// * ``llvm.genx.sumad.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usmad.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uumad.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width
  ///
  def int_genx_ssmad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sumad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usmad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uumad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*max`` : max instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.smax`` : result and operands signed
  /// * ``llvm.genx.umax`` : result and operands unsigned
  /// * ``llvm.genx.fmax`` : result and operands float
  ///
  /// * arg0: first input, any scalar/vector integer/float type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar, vector integer/float type with same
  ///               vector width
  ///
  /// There is no need for a saturating variant of this intrinsic.
  /// Because max cannot overflow, a saturating max can be represented
  /// by this non-saturating max followed by the applicable one of the
  /// saturating trunc intrinsics.
  ///
  def int_genx_smax : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_umax : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_fmax : Intrinsic<[llvm_anyfloat_ty],
                [llvm_anyfloat_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*min`` : min instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.smin`` : result and operands signed
  /// * ``llvm.genx.umin`` : result and operands unsigned
  /// * ``llvm.genx.fmin`` : result and operands float
  ///
  /// * arg0: first input, any scalar/vector integer/float type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer/float type with same
  ///               vector width
  ///
  /// There is no need for a saturating variant of this intrinsic.
  /// Because min cannot overflow, a saturating min can be represented
  /// by this non-saturating min followed by the applicable one of the
  /// saturating trunc intrinsics.
  ///
  def int_genx_smin : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_umin : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_fmin : Intrinsic<[llvm_anyfloat_ty],
                [llvm_anyfloat_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// mod
  /// ^^^
  /// No intrinsic needed as the LLVM IR SRem, URem and FRem instructions
  /// cover vISA functionality
  ///

  /// ``llvm.genx.*mul`` : mul instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmul`` : result signed, operands signed
  /// * ``llvm.genx.sumul`` : result signed, operands unsigned
  /// * ``llvm.genx.usmul`` : result unsigned, operands signed
  /// * ``llvm.genx.uumul`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width, even i64
  ///
  /// For an integer mul where the result and the sources are the same type,
  /// and is not saturating, use the LLVM IR Mul instruction.
  ///
  /// For an fp mul, use the LLVM IR FMul instruction, followed by
  /// llvm.genx.sat if saturation is required.
  ///
  def int_genx_ssmul : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sumul : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usmul : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uumul : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*mul.sat`` : mul instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmul.sat`` : result signed, operands signed
  /// * ``llvm.genx.sumul.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usmul.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uumul.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type with same
  ///               vector width, even i64
  ///
  def int_genx_ssmul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sumul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usmul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uumul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*mulh`` : mulh instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.smulh`` : signed
  /// * ``llvm.genx.umulh`` : unsigned
  ///
  /// * arg0: first input, any scalar/vector i32 type
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_smulh : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_umulh : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// not
  /// ^^^
  /// Intrinsic not needed; use LLVM IR Xor instruction with -1
  ///

  /// or
  /// ^^
  /// Intrinsic not needed; use LLVM IR Or instruction
  ///

  /// ``llvm.genx.pln`` : plane equation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, vector float with a multiple of 8 elements
  /// * arg1: second input value, vector float with twice as many elements as arg0
  ///
  /// * Return value: result, vector float same type as arg0
  ///
  /// The coefficients for the pln operation are arg0[4n+0], arg0[4n+1] and
  /// * arg0[4n+3]. Which n you get for each 8 wide part of the operation is
  /// undefined, because of the effect of legalization in the GenX backend.
  ///
  /// Therefore, to get the correct effect, arg0 should be the result of a
  /// rdregion<0;4,1> (i.e. replicate rows of 4 elements) whose input is a 4
  /// wide vector, so the same arg0[0], arg0[1] and arg0[3] are guaranteed to be
  /// used.
  ///
  def int_genx_pln : Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, llvm_anyfloat_ty], [IntrNoMem]>;

  /// ``llvm.genx.pow`` : power
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector half/float type
  /// * arg1: second input, same type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_pow : Intrinsic<[llvm_anyfloat_ty],
              [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rndd`` : round down
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rndd :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rnde`` : round to even
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rnde :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rndu`` : round up
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rndu :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rndz`` : round to zero
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rndz :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rsqrt`` : reciprocal square root
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rsqrt :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.sad2`` : two-wide sum of absolute differences
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, vector of i8, multiple of 2 wide
  /// * arg1: second input, same type
  ///
  /// * Return value: result, vector of i16 of same vector width
  ///
  /// The vISA spec says that the args and the result can be signed or
  /// unsigned, and saturation is an option. I can't see that any of that makes
  /// any difference to the result, so I have provided just this one intrinsic.
  ///
  def int_genx_sad2 : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.sad2add`` : two-wide sum of absolute differences and add
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, vector of i8, multiple of 2 wide
  /// * arg1: second input, same type
  /// * arg2: third input, vector of i16 of same vector width
  ///
  /// * Return value: result, same type as arg2
  ///
  /// The vISA spec says that the args and the result can be signed or
  /// unsigned, and saturation is an option. I can't see that any of that makes
  /// any difference to the result, so I have provided just this one intrinsic.
  ///
  def int_genx_sad2add : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*sad2add.sat`` : two-wide sum of absolute differences and add, saturated
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssad2add.sat`` : signed arg2 and result
  /// * ``llvm.genx.usad2add.sat`` : unsigned arg2 and result
  ///
  /// * arg0: first input, vector of i8, multiple of 2 wide
  /// * arg1: second input, same type
  /// * arg2: third input, vector of i16 of same vector width
  ///
  /// * Return value: result, same type as arg2
  ///
  def int_genx_ssad2add_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_usad2add_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*shl`` : shl instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssshl`` : result signed, operands signed
  /// * ``llvm.genx.sushl`` : result signed, operands unsigned
  /// * ``llvm.genx.usshl`` : result unsigned, operands signed
  /// * ``llvm.genx.uushl`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width, even i64
  ///
  def int_genx_ssshl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sushl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usshl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uushl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*shl.sat`` : shl instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssshl.sat`` : result signed, operands signed
  /// * ``llvm.genx.sushl.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usshl.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uushl.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type with same
  ///               vector width, even i64
  ///
  def int_genx_ssshl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sushl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usshl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uushl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// shr
  /// ^^^
  /// Intrinsic is not needed. Because shr cannot overflow, an shr that
  /// saturates with a smaller result type than the execution type can be
  /// represented by an LLVM IR Shr instruction then an llvm.genx.sstrunc.sat.
  ///

  /// ``llvm.genx.sin`` : reciprocal square root
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_sin :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.sqrt`` : reciprocal square root
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_sqrt :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.sqrt_ieee`` : reciprocal square root
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_sqrt_ieee :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// No intrinsic for subb as it has two results.

  /// xor : intrinsic not needed; use LLVM IR Xor instruction


  // ---------------------------------
  /// vISA reserved register intrinsics
  /// ---------------------------------

  /// ``llvm.genx.thread.*`` : read thread ID register
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.thread.x`` : read vISA v1 (%thread_x)
  /// * ``llvm.genx.thread.y`` : read vISA v2 (%thread_y)
  ///
  /// * Return value:  i16 the value read
  ///
  def int_genx_thread_x : Intrinsic<[llvm_i16_ty], [], [IntrNoMem]>;
  def int_genx_thread_y : Intrinsic<[llvm_i16_ty], [], [IntrNoMem]>;

  /// ``llvm.genx.local.*`` : read local ID register
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.local.id.x`` : read vISA v3 (%local_id_x)
  /// * ``llvm.genx.local.id.y`` : read vISA v4 (%local_id_y)
  /// * ``llvm.genx.local.size.x`` : read vISA v5 (%local_size_x)
  /// * ``llvm.genx.local.size.y`` : read vISA v6 (%local_size_y)
  ///
  /// * Return value:  i32 the value read
  ///
  def int_genx_local_id_x : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_local_id_y : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_local_size_x : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_local_size_y : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;

  /// ``llvm.genx.group.*`` : read group ID register
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// ``llvm.genx.group.id.x`` : read vISA v7 (%local_id_x)
  /// ``llvm.genx.group.id.y`` : read vISA v8 (%local_id_y)
  /// ``llvm.genx.group.count.x`` : read vISA v9 (%local_id_x)
  /// ``llvm.genx.group.count.y`` : read vISA v10 (%local_id_y)
  ///
  /// * Return value:  i32 the value read
  ///
  def int_genx_group_id_x : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_group_id_y : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_group_count_x : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_group_count_y : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;

  /// ``llvm.genx.timestamp`` : read vISA v11 (%timestamp)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * Return value:  vxi32 the value read
  ///
  /// The vector width must be power of 2 and no larger than 4.
  ///
  def int_genx_timestamp : Intrinsic<[llvm_anyint_ty], [], []>;

  /// ``llvm.genx.r0`` : read vISA v12 (%r0)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * Return value:  vxi32 or i32 the value read
  ///
  /// The vector width must be power of 2 and no larger than 8.
  ///
  def int_genx_r0 : Intrinsic<[llvm_anyint_ty], [], [IntrReadMem]>;

  /// ``llvm.genx.get.color`` : read color value of the thread origin
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// Return Value: i16 the value read
  ///
  /// This may not be the most appropriate way to access this value,
  /// but is a stop-gap solution.
  ///
  def int_genx_get_color : Intrinsic<[llvm_i16_ty], [], [IntrNoMem]>;

  // --------------------------
  /// Shared function intrinsics
  /// --------------------------
  /// These are in the order they appear in the vISA spec, not in
  /// alphabetical order.
  ///

  /// ``llvm.genx.dword.atomic.*`` : dword atomic with binary operator
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.dword.atomic.add`` : vISA DWORD_ATOMIC ADD instruction
  /// * ``llvm.genx.dword.atomic.sub`` : vISA DWORD_ATOMIC SUB instruction
  /// * ``llvm.genx.dword.atomic.min`` : vISA DWORD_ATOMIC MIN instruction
  /// * ``llvm.genx.dword.atomic.max`` : vISA DWORD_ATOMIC MAX instruction
  /// * ``llvm.genx.dword.atomic.xchg`` : vISA DWORD_ATOMIC XCHG instruction
  /// * ``llvm.genx.dword.atomic.and`` : vISA DWORD_ATOMIC AND instruction
  /// * ``llvm.genx.dword.atomic.or`` : vISA DWORD_ATOMIC OR instruction
  /// * ``llvm.genx.dword.atomic.xor`` : vISA DWORD_ATOMIC XOR instruction
  /// * ``llvm.genx.dword.atomic.imin`` : vISA DWORD_ATOMIC IMIN instruction
  /// * ``llvm.genx.dword.atomic.imax`` : vISA DWORD_ATOMIC IMAX instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXi32 src
  /// * arg4: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_add : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_sub : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_min : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_max : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_xchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_and : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_or : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_xor : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_imin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_imax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.*`` : dword atomic with fmin/fmax operation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.dword.atomic.fmin`` : vISA DWORD_ATOMIC FMIN instruction
  /// * ``llvm.genx.dword.atomic.fmax`` : vISA DWORD_ATOMIC FMAX instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXfloat src
  /// * arg4: vXfloat original value of the register that the data is read into
  ///
  /// * Return value: vXfloat the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_fmin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_fmax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.*`` : dword atomic with inc/dec operation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.dword.atomic.inc`` : vISA DWORD_ATOMIC INC instruction
  /// * ``llvm.genx.dword.atomic.dec`` : vISA DWORD_ATOMIC DEC instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_inc : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_dec : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.cmpxchg`` : vISA DWORD_ATOMIC CMPXCHG instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXi32 src0
  /// * arg4: vXi32 src1
  /// * arg5: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_cmpxchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.fcmpwr`` : vISA DWORD_ATOMIC FCMPWR instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXfloat src0
  /// * arg4: vXfloat src1
  /// * arg5: vXfloat original value of the register that the data is read into
  ///
  /// * Return value: vXfloat the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_fcmpwr : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.gather`` : vISA GATHER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: Elt_size inferred from argument type
  /// * arg1: i32 is_modified, constant
  /// * (Num_elts inferred from data type)
  /// * arg2: i32 surface index
  /// * arg3: i32 global offset in elements
  /// * arg4: vXi32 element offset in elements
  /// * arg5: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the return value is the number of elements to read,
  /// which must be 1, 8 or 16.
  ///
  /// The element offset arg must have the same vector width.
  ///
  def int_genx_gather : Intrinsic<[llvm_anyvector_ty], [llvm_anyvector_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>],
    [IntrReadMem]>;

  /// ``llvm.genx.gather.scaled`` : vISA GATHER_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  ///       (block size MBZ, means 1 byte)
  /// * arg1: i32 log2 num blocks, constant (0/1/2 for num blocks 1/2/4)
  /// * arg2: i32 surface index
  /// * arg3: i16 scale, constant
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset in bytes (X = 8 or 16)
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  ///
  /// The predicate arg must have the same vector width.
  ///
  /// The block size must be 1 byte.
  ///
  /// Only T0 (SLM) and T5 (stateless) are supported.
  ///
  /// The old value of the data read (the return value) must have UD, D or
  /// F type. For 1 and 2 byte (1 x num blocks) reads the upper bytes have
  /// undefined values in the returned value.
  ///
  /// This instruction is available for SKL+ in general and it works for pre-SKL
  /// only when scale is 0.
  ///
  def int_genx_gather_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, llvm_i16_ty, llvm_i32_ty,
    llvm_anyint_ty, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.gather4`` : vISA GATHER4 instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: i32 is_modified, constant
  /// * arg2: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg3: i32 surface index
  /// * arg4: i32 global offset in i32s
  /// * arg5: vXi32 element offset in i32s
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_gather4 : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty,
    LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.gather4.scaled`` : vISA GATHER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg2: i32 surface index
  /// * arg3: i16 scale, constant
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset in bytes
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must have the same vector width.
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_gather4_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, llvm_i16_ty, llvm_i32_ty,
    llvm_anyint_ty, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.gather4.typed`` : vISA GATHER4_TYPED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg2: i32 surface index
  /// * arg3: vXi32 U pixel address
  /// * arg4: vXi32 V pixel address
  /// * arg5: vXi32 R pixel address
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal and are the number of elements to read, which
  /// must be 8.
  /// The predicate arg must have the same vector width.
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels to read.
  /// The number of 1 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element. Mask "0000" is not allowed.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_gather4_typed : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyvector_ty, llvm_i32_ty, llvm_v8i32_ty, llvm_v8i32_ty,
    llvm_v8i32_ty, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.media.ld`` : vISA MEDIA_LD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 modifiers, constant
  /// * arg1: i32 surface index
  /// * arg2: i32 plane, constant
  /// * arg3: i32 block width in bytes, constant
  /// * (block height inferred from return type size and block width)
  /// * arg4: i32 x byte offset
  /// * arg5: i32 y byte offset
  ///
  /// * Return value: the data read.
  ///
  /// The number of bytes taken by a row in the return value, the "rounded
  /// block width", is the block width rounded up to the next power of two
  /// no less than 4. The size of the return type must be a multiple of
  /// this rounded block width, and the multiplier is the block height.
  ///
  /// The block width has a maximum of 32 (64 on BDW+). The maxmimum byte
  /// size of the return type is 256.
  ///
  def int_genx_media_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.media.st`` : vISA MEDIA_ST instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 modifiers, constant
  /// * arg1: i32 surface index
  /// * arg2: i32 plane, constant
  /// * arg3: i32 block width in bytes, constant
  /// * (block height inferred from data type size and block width)
  /// * arg4: i32 x byte offset
  /// * arg5: i32 y byte offset
  /// * arg6: data to write
  ///
  /// The number of bytes taken by a row in the return value, the "rounded
  /// block width", is the block width rounded up to the next power of two
  /// no less than 4. The size of the data to write type must be a multiple of
  /// this rounded block width, and the multiplier is the block height.
  ///
  /// The block width has a maximum of 32 (64 on BDW+). The maxmimum byte
  /// size of the data to write is 256.
  ///
  def int_genx_media_st : Intrinsic<[], [llvm_i32_ty, llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.oword.ld*`` : oword load instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.oword.ld`` : vISA OWORD_LD instruction
  /// * ``llvm.genx.oword.ld.unaligned`` : vISA OWORD_LD_UNALIGNED instruction
  ///
  /// * (log2 number of owords inferred from return type)
  /// * arg0: i32 is_modified, constant
  /// * arg1: i32 surface index
  /// * arg2: i32 offset (in owords for .ld / in bytes for .ld.unaligned)
  ///
  /// * Return value: the data read.
  ///
  /// The byte size of the return type must be 16, 32, 64, or 128.
  ///
  def int_genx_oword_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty], [IntrReadMem]>;
  def int_genx_oword_ld_unaligned : Intrinsic<[llvm_anyvector_ty],
    [llvm_i32_ty, llvm_i32_ty, llvm_i32_ty], [IntrReadMem]>;

  /// ``llvm.genx.oword.st`` : vISA OWORD_ST instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (log2 number of owords inferred from return type)
  /// * arg0: i32 surface index
  /// * arg1: i32 offset (in owords)
  /// * arg2: data to write
  ///
  /// The byte size of the data to write must be 16, 32, 64, or 128.
  ///
  def int_genx_oword_st :
    Intrinsic<[], [llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter`` : vISA SCATTER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: Elt_size from argument element type
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in elements
  /// * arg3: vXi32 element offset in elements
  /// * arg4: the data to write. The first <num_elts> elements will be used.
  ///
  /// The operand must have one of UD, D, F type; for 1 and 2 byte accesses the upper
  /// bits will be ignored.
  ///
  /// The vector width of the data to write is the number of elements to write,
  /// which must be 1, 8 or 16.
  /// The element offset arg must have the same vector width.
  ///
  def int_genx_scatter : Intrinsic<[], [llvm_anyvector_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_anyint_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter.scaled`` : vISA SCATTER_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  ///       (block size MBZ, means 1 byte)
  /// * arg1: i32 log2 num blocks, constant (0/1/2 for num blocks 1/2/4)
  /// * arg2: i32 surface index
  /// * arg3: i16 scale, constant
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset (X = 8 or 16)
  /// * arg6: data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// write, which must be 8 or 16.
  ///
  /// The predicate arg must have the same vector width.
  ///
  /// The block size must be 1 byte.
  ///
  /// Only T0 (SLM) and T5 (stateless) are supported.
  ///
  /// The data type to write must have UD, D or F type. For 1 and 2 byte (1 x num
  /// blocks) accesses the upper bytes will be ignored.
  ///
  /// This instruction is available for SKL+ in general and it works for pre-SKL
  /// only when scale is 0.
  ///
  def int_genx_scatter_scaled : Intrinsic<[], [llvm_anyvector_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i16_ty, llvm_i32_ty, llvm_anyint_ty,
    llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter4`` : vISA SCATTER4 instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg2: i32 surface index
  /// * arg3: i32 global offset in i32s
  /// * arg4: vXi32 element offset in i32s
  /// * arg5: the data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must either have the same vector width, or be a scalar
  /// i1 constant with value 1.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to write.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element.
  /// The vector width of the data to write arg must be the number of elements
  /// times the number of channels to write per element.
  /// The element type of the data to write must be i32 or float.
  ///
  def int_genx_scatter4 : Intrinsic<[], [llvm_i32_ty, llvm_anyvector_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter4.scaled`` : vISA SCATTER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg2: i32 surface index
  /// * arg3: i16 scale, constant
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset in bytes
  /// * arg6: data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// write, which must be 8 or 16.
  /// The predicate arg must have the same vector width.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element.
  /// The channels to write must be contiguous and starting at channel 0.
  /// The vector width of the data to write must be the number of elements
  /// times the number of channels to write per element.
  /// The element type of the data to write must be i32 or float.
  ///
  def int_genx_scatter4_scaled : Intrinsic<[], [llvm_anyvector_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_i16_ty, llvm_i32_ty, llvm_anyint_ty, llvm_anyvector_ty],
    []>;

  /// ``llvm.genx.scatter4.typed`` : vISA SCATTER4_TYPED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: vXi1 predicate (Num_elts inferred from U pixel address type)
  /// * arg2: i32 surface index
  /// * arg3: v8Xi32 U pixel address
  /// * arg4: v8Xi32 V pixel address
  /// * arg5: v8Xi32 R pixel address
  /// * arg6: data to write
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal and are the number of elements to write, which
  /// must be 8.
  /// The predicate arg must have the same vector width.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels to write.
  /// The number of 1 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element. Mask "0000" is not allowed.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the source value must be i32 or float.
  ///
  def int_genx_scatter4_typed : Intrinsic<[], [llvm_i32_ty, llvm_anyvector_ty,
    llvm_i32_ty, llvm_v8i32_ty, llvm_v8i32_ty, llvm_v8i32_ty,
    llvm_anyvector_ty], []>;

  /// ``llvm.genx.transpose.ld`` : vISA TRANSPOSE_LD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: i32 log2 block width in i32s, constant (0-3)
  /// * (log2 block height inferred from block width and data type, 0-3)
  /// * arg2: i32 X offset
  /// * arg3: i32 Y offset
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the return value is the number of elements to read.
  /// This must be a multiple of the block width. The block height is then
  /// inferred from those values.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_transpose_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty], [IntrReadMem]>;

  /// ``llvm.genx.untyped.atomic.*`` : vISA UNTYPED_ATOMIC with binary operator
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.untyped.atomic.add`` : vISA UNTYPED_ATOMIC ADD instruction
  /// * ``llvm.genx.untyped.atomic.sub`` : vISA UNTYPED_ATOMIC SUB instruction
  /// * ``llvm.genx.untyped.atomic.min`` : vISA UNTYPED_ATOMIC MIN instruction
  /// * ``llvm.genx.untyped.atomic.max`` : vISA UNTYPED_ATOMIC MAX instruction
  /// * ``llvm.genx.untyped.atomic.xchg`` : vISA UNTYPED_ATOMIC XCHG instruction
  /// * ``llvm.genx.untyped.atomic.and`` : vISA UNTYPED_ATOMIC AND instruction
  /// * ``llvm.genx.untyped.atomic.or`` : vISA UNTYPED_ATOMIC OR instruction
  /// * ``llvm.genx.untyped.atomic.xor`` : vISA UNTYPED_ATOMIC XOR instruction
  /// * ``llvm.genx.untyped.atomic.imin`` : vISA UNTYPED_ATOMIC IMIN instruction
  /// * ``llvm.genx.untyped.atomic.imax`` : vISA UNTYPED_ATOMIC IMAX instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in i32s
  /// * arg3: vXi32 element offset in i32s
  /// * arg4: vXi32 src
  /// * arg5: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  //same vector / width, which must be 8 or 16.
  ///
  def int_genx_untyped_atomic_add : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_sub : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_min : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_max : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_xchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_and : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_or : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_xor : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_imin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_imax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.untyped.atomic.*`` : vISA UNTYPED_ATOMIC with inc/dec
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.untyped.atomic.inc`` : vISA UNTYPED_ATOMIC INC instruction
  /// * ``llvm.genx.untyped.atomic.dec`` : vISA UNTYPED_ATOMIC DEC instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in i32s
  /// * arg3: vXi32 element offset in i32s
  /// * arg4: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset and the return value must have the same vector
  /// width, which must be 8 or 16.
  ///
  def int_genx_untyped_atomic_inc : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_dec : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.untyped.atomic.cmpxchg`` : vISA UNTYPED_ATOMIC CMPXCHG instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in i32s
  /// * arg3: vXi32 element offset in i32s
  /// * arg4: vXi32 src0
  /// * arg5: vXi32 src1
  /// * arg6: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src0, src1, and the return value must all have
  /// the same vector width, which must be 8 or 16.
  ///
  def int_genx_untyped_atomic_cmpxchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.svm.block.ld*`` : vISA SVM BLOCK_LD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.svm.block.ld`` : vISA SVM BLOCK_LD instruction with oword alignment
  /// * ``llvm.genx.svm.block.ld.unaligned`` : vISA SVM BLOCK_LD instruction with
  ///   dword alignment
  ///
  /// * (log2 number of oword inferred from data type)
  /// * arg0: i64 address
  ///
  /// * Return value: data read
  ///
  /// The data read must have a size that is a power of two from 16 to 128
  /// bytes.
  ///
  def int_genx_svm_block_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i64_ty], []>;
  def int_genx_svm_block_ld_unaligned : Intrinsic<[llvm_anyvector_ty],
    [llvm_i64_ty], [IntrReadMem]>;

  /// ``llvm.genx.svm.block.st`` : vISA SVM BLOCK_ST instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (log2 number of oword inferred from data type)
  /// * arg0: i64 address
  /// * arg1: data to write
  ///
  /// The data to write must have a size that is a power of two from 16 to 128
  /// bytes.
  ///
  def int_genx_svm_block_st : Intrinsic<[llvm_anyvector_ty], [llvm_i64_ty], []>;

  /// ``llvm.genx.svm.gather`` : vISA SVM GATHER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec size inferred from address vector width)
  /// * arg0: vXi1 predicate (Num_elts inferred from this arg)
  /// * (block size inferred from data element type)
  /// * arg1: i32 log2 num blocks, constant (0/1/2/3 for num blocks 1/2/4/8)
  /// * arg2: vXi64 address (X = 8 or 16)
  /// * arg3: old value of the data read
  /// 
  /// * Return value: data read
  ///
  /// The return value element type is i8 for block size 1, i32/float for
  /// block size 4, or i64/double for block size 8.
  /// The return value vector width is the address vector width times
  /// number of blocks (rounded up to 4 if block size is 1).
  ///
  def int_genx_svm_gather : Intrinsic<[llvm_anyvector_ty], [llvm_anyvector_ty,
    llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.svm.gather4.scaled`` : vISA SVM GATHER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg3: i16 scale, constant
  /// * arg4: i64 global address in bytes
  /// * arg5: vXi64 element offset in bytes
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must either have the same vector width, or be a scalar
  /// i1 constant with value 1.
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_svm_gather4_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i64_ty, llvm_anyint_ty,
    LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.svm.scatter`` : vISA SVM SCATTER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec size inferred from address vector width)
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * (block size inferred from data element type)
  /// * arg1: i32 log2 num blocks, constant (0/1/2/3 for num blocks 1/2/4/8)
  /// * arg2: vXi64 address (X = 8 or 16)
  /// * arg3: data to write
  ///
  /// The data to write element type is i8 for block size 1, i32/float for
  /// block size 4, or i64/double for block size 8.
  /// The data vector width is the address vector width times
  /// number of blocks (rounded up to 4 if block size is 1).
  ///
  def int_genx_svm_scatter : Intrinsic<[], [llvm_anyvector_ty, llvm_i32_ty,
    llvm_anyint_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.svm.scatter4.scaled`` : vISA SVM SCATTER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg3: i16 scale, constant
  /// * arg4: i64 global address in bytes
  /// * arg5: vXi64 element offset in bytes
  /// * arg6: data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must either have the same vector width, or be a scalar
  /// i1 constant with value 1.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element.
  /// The vector width of the data to write arg must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the data to write arg must be i32 or float.
  ///
  def int_genx_svm_scatter4_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i64_ty, llvm_anyint_ty],
    []>;

  /// ``llvm.genx.svm.atomic.*`` : vISA SVM_ATOMIC with binary operator
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.svm.atomic.add`` : vISA SVM_ATOMIC ADD instruction
  /// * ``llvm.genx.svm.atomic.sub`` : vISA SVM_ATOMIC SUB instruction
  /// * ``llvm.genx.svm.atomic.min`` : vISA SVM_ATOMIC MIN instruction
  /// * ``llvm.genx.svm.atomic.max`` : vISA SVM_ATOMIC MAX instruction
  /// * ``llvm.genx.svm.atomic.xchg`` : vISA SVM_ATOMIC XCHG instruction
  /// * ``llvm.genx.svm.atomic.and`` : vISA SVM_ATOMIC AND instruction
  /// * ``llvm.genx.svm.atomic.or`` : vISA SVM_ATOMIC OR instruction
  /// * ``llvm.genx.svm.atomic.xor`` : vISA SVM_ATOMIC XOR instruction
  /// * ``llvm.genx.svm.atomic.imin`` : vISA SVM_ATOMIC IMIN instruction
  /// * ``llvm.genx.svm.atomic.imax`` : vISA SVM_ATOMIC IMAX instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from this arg)
  /// * arg1: vXi64 element addresses in bytes
  /// * arg2: vXi32/vXi64 src
  /// * arg3: original value of the register that the data is read into
  ///
  /// * Return value: vXi32/vXi64 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 8 (BDW+) or 16 (SKL+).
  ///
  def int_genx_svm_atomic_add : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_sub : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_min : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_max : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_xchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_and : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_or : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_xor : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_imin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_imax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.svm.atomic.*`` : vISA SVM_ATOMIC with inc/dec
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.svm.atomic.inc`` : vISA SVM_ATOMIC INC instruction
  /// * ``llvm.genx.svm.atomic.dec`` : vISA SVM_ATOMIC DEC instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from this arg)
  /// * arg1: vXi64 element addresses in bytes
  /// * arg2: vXi32/vXi64 src
  /// * arg3: original value of the register that the data is read into
  ///
  /// * Return value: vXi32/vXi64 the old value read
  ///
  /// Predicate, element offset and the return value must have the same vector
  /// width, which must be 8 (BDW+) or 16 (SKL+).
  ///
  def int_genx_svm_atomic_inc : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_dec : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>], []>;

  /// ``llvm.genx.svm.atomic.cmpxchg`` : vISA SVM_ATOMIC CMPXCHG instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: vXi64 element addresses in bytes
  /// * arg2: vXi32/vXi64 src0
  /// * arg3: vXi32/vXi64 src1
  /// * arg4: original value of the register that the data is read into
  ///
  /// * Return value: vXi32/vXi64 the old value read
  ///
  /// Predicate, element offset, src0, src1, and the return value must all have
  /// the same vector width, which must be 8 (BDW+) or 16 (SKL+).
  ///
  def int_genx_svm_atomic_cmpxchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.load`` : vISA LOAD (sampler load) instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant (simd_mode inferred from pixel address operands)
  /// * arg1: i32 surface index
  /// * arg2: vXi32 U pixel address
  /// * arg3: vXi32 V pixel address
  /// * arg4: vXi32 R pixel address
  ///
  /// * Return value: the data read
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal, and either 8 or 16.
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  ///
  /// For SIMD8 pre-BDW, the vector width of the data read must be 32.
  /// For SIMD8 BDW+, or for SIMD16, the vector width of the data read must be
  /// the SIMD width times the number of enabled channels.
  ///
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_load : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrReadMem]>;

  /// ``llvm.genx.sample`` : vISA SAMPLE instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant (simd_mode inferred from pixel address operands)
  /// * arg1: i32 sampler index
  /// * arg2: i32 surface index
  /// * arg3: vXfloat U pixel address
  /// * arg4: vXfloat V pixel address
  /// * arg5: vXfloat R pixel address
  ///
  /// * Return value: the data read
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal, and either 8 or 16.
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  ///
  /// For SIMD8 pre-BDW, the vector width of the data read must be 32.
  /// For SIMD8 BDW+, or for SIMD16, the vector width of the data read must be
  /// the SIMD width times the number of enabled channels.
  ///
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_sample : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyfloat_ty, LLVMMatchType<1>,
    LLVMMatchType<1>], [IntrReadMem]>;

  /// ``llvm.genx.sample.unorm`` : vISA SAMPLE_UNORM instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: i32 sampler index
  /// * arg2: i32 surface index
  /// * arg3: float U pixel address
  /// * arg4: float V pixel address
  /// * arg5: float DeltaU
  /// * arg6: float DeltaV
  ///
  /// * Return value: v8i16 the data read
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  ///
  def int_genx_sample_unorm : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.avs`` : vISA AVS instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: i32 sampler index
  /// * arg2: i32 surface index
  /// * arg3: float U offset
  /// * arg4: float V offset
  /// * arg5: float deltaU
  /// * arg6: float deltaV
  /// * arg7: float u2d
  /// * arg8: i32 groupID
  /// * arg9: i32 verticalBlockNumber
  /// * arg10: i32 Output format control, constant
  /// * arg11: float v2d
  /// * arg12: i32 execMode, constant
  /// * arg13: i8 IEFBypass
  ///
  /// * Return value: the data read.
  ///
  /// The actual data returned is determined by a combination of <channel>,
  /// <cntrl>, <execMode>, as well as whether output shuffle is enabled in the
  /// sampler state.
  ///
  /// SIMD Control Flow: channel enable is ignored.
  ///
  def int_genx_avs : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty,
    llvm_float_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_float_ty,
    llvm_i32_ty, llvm_i8_ty], [IntrReadMem]>;

  /// ``llvm.genx.barrier`` : vISA BARRIER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  def int_genx_barrier : Intrinsic<[],[],[]>;

  /// ``llvm.genx.cache.flush`` : vISA CACHE_FLUSH instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  def int_genx_cache_flush : Intrinsic<[],[],[]>;

  /// ``llvm.genx.fence`` : vISA FENCE instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i8 mask, constant
  ///
  def int_genx_fence : Intrinsic<[], [llvm_i8_ty], []>;

  /// ``llvm.genx.wait`` : vISA WAIT instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i8 thread mask
  ///
  def int_genx_wait : Intrinsic<[], [llvm_i8_ty], []>;

  /// ``llvm.genx.yield`` : vISA YIELD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  def int_genx_yield : Intrinsic<[],[],[]>;

  /// ``llvm.genx.raw.send`` : vISA RAW_SEND instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor, constant
  /// * (numsrc inferred from src size)
  /// * (numdst inferred from dst size)
  /// * arg3: i32 desc
  /// * arg4: src
  /// * arg5: old_dst
  ///
  /// * Return value: dst
  ///
  /// The SEND instruction has a field for the size of each of src
  /// and dst. These are inferred by rounding the size of each of src and
  /// dst up to the next whole GRF.
  ///
  /// If the send writes to the whole of dst, or the program does not care what
  /// was in those registers before, then set old_dst to UndefValue (of the same
  /// type as dst). If on the other hand the send is predicated and the program
  /// needs to see what was in the parts of destination registers not written
  /// by the send, then use old_dst as the "old value of destination registers"
  /// input.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_send : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyint_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.raw.send.noresult`` : vISA RAW_SEND instruction with no result
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor, constant
  /// * (numsrc inferred from src size)
  ///       (numdst is 0)
  /// * arg3: i32 desc
  /// * arg4: src
  ///
  /// The SEND instruction has a field for the size of src. This is inferred by
  /// rounding the size of src up to the next whole GRF.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_send_noresult : Intrinsic<[], [llvm_i32_ty, llvm_anyint_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.raw.sends`` : vISA RAW_SENDS instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor, constant
  /// * (numsrc inferred from src size)
  /// * (numsrc2 inferred from src2 size)
  /// * (numdst inferred from dst size)
  /// * arg3: i32 desc
  /// * arg4: src
  /// * arg5: src2
  /// * arg6: old_dst
  ///
  /// * Return value: dst
  ///
  /// The SENDS instruction has a field for the size of each of src, src2
  /// and dst. These are inferred by rounding the size of each of src, src2 and
  /// dst up to the next whole GRF.
  ///
  /// If the send writes to the whole of dst, or the program does not care what
  /// was in those registers before, then set old_dst to UndefValue (of the same
  /// type as dst). If on the other hand the send is predicated and the program
  /// needs to see what was in the parts of destination registers not written
  /// by the send, then use old_dst as the "old value of destination registers"
  /// input.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_sends : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyint_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, LLVMMatchType<0>], []>;

  /// ``llvm.genx.raw.sends.noresult`` : vISA RAW_SENDS instruction with no result
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor, constant
  /// * (numsrc inferred from src size)
  /// * (numsrc2 inferred from src2 size)
  /// * (numdst is 0)
  /// * arg3: i32 desc
  /// * arg4: src
  /// * arg5: src2
  ///
  /// The SENDS instruction has a field for the size of each of src and src2.
  /// These are inferred by rounding the size of each of src and src2 up to the
  /// next whole GRF.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_sends_noresult : Intrinsic<[], [llvm_i32_ty, llvm_anyint_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty, llvm_anyvector_ty], []>;

  //--------------------------------------------------------------------
  // GenX backend internal intrinsics

  // llvm.genx.constanti : copy constant to register
  // llvm.genx.constantf : copy constant to register
  //
  // arg0: input value (constant, any scalar or vector type other than i1 or
  //         vector of i1)
  //
  // Return value: same type
  //
  // This intrinsic is inserted by the GenXLowering pass
  // to load a constant in a way that stops the subsequent CSE pass
  // from propagating it back into the operand using it.
  //
  // There are two variants simply because there is no way of saying here
  // that an argument can have any scalar or vector type.
  //
  def int_genx_constanti : Intrinsic<[llvm_anyint_ty],
          [LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_constantf : Intrinsic<[llvm_anyfloat_ty],
          [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.convert : convert register category (non address)
  //
  // arg0: input value (i32 or vector of i32)
  //
  // Return value: converted value (same type)
  //
  // This intrinsic is inserted by the GenXCatgory pass to represent
  // a value being converted between two register categories. The input and
  // result categories are not represented; they are implied by the other
  // def/uses of the value. Address conversion is not covered by this
  // intrinsic.
  //
  // The intrinsic is also inserted by GenXCoalescing to represent a copy
  // of a value of category other than general. Thus the input and output
  // might be both the same category, but not both general.
  //
  def int_genx_convert :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.convert.addr : convert to address register category
  //
  // arg0: input value (i16 or vector of i16)
  //
  // Return value: converted value (same type)
  //
  // This intrinsic is inserted by the GenXCatgoryConversion pass to represent
  // a value being converted from a general value to an address, used as the
  // variable index in an element or region access.
  //
  def int_genx_convert_addr :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.constantpred : load constant predicate (i1 or vector of i1)
  //
  // arg0: constant i1 or vector of i1
  //
  // Return value: loaded value, same type
  //
  // This intrinsic is inserted by GenXLowering to load a predicate constant.
  // We could just use a bitcast, except that EarlyCSE follows
  // GenXConstantMaterialization and it has a habit of putting the constant
  // back in the wrregion.
  def int_genx_constantpred :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.add.addr : add an offset onto an address register
  //
  // arg0: lhs input (i16 or vector of i16)
  // arg1: rhs input (same type)
  //
  // Return value: result of add (same type)
  //
  // When the result of a constant add/sub is used as a variable index in
  // a region access, GenXCategoryConversion converts it into this intrinsic
  // so that it will be considered an add to an address register.
  //
  def int_genx_add_addr :
            Intrinsic<[llvm_anyint_ty],
                [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.rdpredregion : read region at specified offset from a predicate
  //
  // arg0: i1 vector
  // arg1: constant i32 offset (in elements)
  //
  // Return value: v4i1/v8i1/v16i1 result of region read
  //
  // The number of elements to read is determined from the number of elements
  // in the return type, and must be 4, 8 or 16.
  // The offset must be a multiple of the number of elements.
  //
  def int_genx_rdpredregion : Intrinsic<[llvm_anyint_ty],
      [llvm_anyint_ty, llvm_i32_ty], [IntrNoMem]>;

  // llvm.genx.wrpredregion : write region at specified offset into a predicate
  //
  // arg0: i1 old value of vector
  // arg1: i1 subvector to write into region
  // arg2: constant i32 offset (in elements)
  //
  // Return value: v4i1/v8i1/v16i1 result of region write
  //
  // The number of elements to write is determined from the number of elements
  // in the "subvector to write" arg, and must be 4, 8 or 16.
  // The offset must be a multiple of the number of elements.
  //
  def int_genx_wrpredregion : Intrinsic<[llvm_anyint_ty],
      [LLVMMatchType<0>, llvm_anyint_ty, llvm_i32_ty], [IntrNoMem]>;

  // llvm.genx.noti : invert bits in integer or predicate
  // llvm.genx.notp : invert bits in predicate
  //
  // arg0: input value, scalar/vector
  //
  // Return value: result, same type
  //
  // GenXLowering lowers xor with -1 into one of these intrinsics.
  //
  def int_genx_noti :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_notp :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.simdcf.any : simd cf marker intrinsic.
  //
  // arg0: vector of i1
  // arg1: filename string (global constant)
  // arg2: line number (constant)
  //
  // Return value: i1 value as condition for a scalar control flow.
  //
  // This intrinsic is used to mark a simd cf that takes a predicate vector and
  // returns a scalar value for scalar cf. The last two arguments can be used to
  // generate debug location for the corresponding call site.
  //
  def int_genx_simdcf_any :
              Intrinsic<[llvm_i1_ty],
                        [llvm_anyvector_ty, llvm_ptr_ty, llvm_i32_ty], []>;
}
